{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import json\n",
    "import numpy as np\n",
    "# 数据加载和基础预处理\n",
    "path = Path(r'C:\\Users\\matrix\\Desktop\\kaggle\\芒果tv\\Data_A')\n",
    " # 'did_features'\n",
    " # 'vid_info'\n",
    " # '弹幕文本数据',\n",
    " # '用户历史播放数据'\n",
    " # '用户历史曝光数据'\n",
    " # '用户历史点击数据'\n",
    " # 'A榜待预测的did'\n",
    " # 'A榜用户曝光数据'\n",
    "def read_log_data(data_type='view',day=1,read_type='read'):\n",
    "    \n",
    "    \n",
    "    data_path = path \n",
    "    \n",
    "    # data_path = path\n",
    "    \n",
    "    if day<10:\n",
    "        date = '0'+str(day)\n",
    "    else:\n",
    "        date = str(day)\n",
    "    \n",
    "    if data_type == 'view':\n",
    "        data_path = data_path / '用户历史曝光数据'/ f'day{date}' / f'did_show_data{date}.csv'\n",
    "    elif data_type == 'click':\n",
    "         data_path = data_path /'用户历史点击数据'/ f'day{date}' / f'day{date}_data.csv'\n",
    "    elif data_type == 'play':\n",
    "        data_path = data_path /'用户历史播放数据'/ f'day{date}' / f'day{date}_data.csv'\n",
    "    else:\n",
    "        raise NameError(f'no such data type \"{data_type}\"')\n",
    "        \n",
    "    if read_type=='read':\n",
    "        return pl.read_csv(data_path)\n",
    "    elif read_type=='scan':\n",
    "        return pl.scan_csv(data_path)\n",
    "    else:\n",
    "        raise NameError(f'no such read type \"{data_type}\"')\n",
    "\n",
    "def scan_log_data(start_day,end_day):\n",
    "    scan_data_all  = {'view':{},'click':{},'play':{}}\n",
    "\n",
    "    for day_i in range(start_day,end_day):\n",
    "        scan_data_all['view'][day_i] =  read_log_data(data_type='view',day=day_i,read_type='scan')\n",
    "        scan_data_all['click'][day_i] =  read_log_data(data_type='click',day=day_i,read_type='scan')\n",
    "        scan_data_all['play'][day_i] =  read_log_data(data_type='play',day=day_i,read_type='scan')\n",
    "    return scan_data_all\n",
    "def load_log_data(start_day,end_day):\n",
    "    data_all  = {'view':{},'click':{},'play':{}}\n",
    "\n",
    "    for day_i in range(start_day,end_day):\n",
    "        data_all['view'][day_i] =  read_log_data(data_type='view',day=day_i,read_type='read')\n",
    "        data_all['click'][day_i] =  read_log_data(data_type='click',day=day_i,read_type='read')\n",
    "        data_all['play'][day_i] =  read_log_data(data_type='play',day=day_i,read_type='read')\n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_data():\n",
    "    user_data = pl.read_csv(path/'did_features/did_features_table.csv')\n",
    "    (path/'mappings').mkdir(exist_ok=True)\n",
    "    \n",
    "    cat_columns = [\n",
    "        col for col in user_data.columns \n",
    "        if (col not in ('did', 'f87')) and (user_data[col].n_unique() < len(user_data)*0.3 and (user_data[col] == user_data[col].cast(pl.Int64)).all())\n",
    "    ]\n",
    "    num_columns = [col for col in user_data.columns if col not in cat_columns and col != 'did']\n",
    "    # 填充枚举类特征\n",
    "    if cat_columns:\n",
    "        user_data = user_data.with_columns([\n",
    "            pl.col(col).fill_null(-1.0).cast(pl.Int64)\n",
    "            for col in cat_columns\n",
    "        ])\n",
    "    cat_columns.append('f87')\n",
    "    \n",
    "    mapping_dicts = {}\n",
    "    for col in cat_columns:\n",
    "        # 生成映射字典\n",
    "        unique_values = user_data[col].unique().to_list()\n",
    "        mapping = {v: i for i, v in enumerate(unique_values)}\n",
    "        mapping_dicts[col] = mapping\n",
    "        \n",
    "        # 应用映射并保存\n",
    "        user_data = user_data.with_columns(\n",
    "            pl.col(col).map_elements(lambda x: mapping.get(x, -1), return_dtype=pl.Int64).alias(col)\n",
    "        )\n",
    "        with open(path/f'mappings/{col}_mapping.json', 'w') as f:\n",
    "            json.dump(mapping, f)\n",
    "\n",
    "    # 填充数值类特征\n",
    "    if num_columns:\n",
    "        user_data = user_data.with_columns([\n",
    "            pl.col(col).fill_null(pl.col(col).mean()).cast(pl.Float32)  # 填充均值并转换为Float32类型\n",
    "            for col in num_columns\n",
    "        ]\n",
    "    )\n",
    "    norm_params = {}\n",
    "    for col in num_columns:\n",
    "        # 计算均值和标准差\n",
    "        mean_val = user_data[col].mean()\n",
    "        std_val = user_data[col].std()\n",
    "        norm_params[col] = {'mean': mean_val, 'std': std_val}\n",
    "        \n",
    "        # 应用标准化\n",
    "        user_data = user_data.with_columns(\n",
    "            ((pl.col(col) - mean_val) / std_val).alias(col)\n",
    "        )\n",
    "    # 保存正则化参数\n",
    "    with open(path/'mappings/normalization_params.json', 'w') as f:\n",
    "        json.dump(norm_params, f)\n",
    "\n",
    "    return user_data\n",
    "\n",
    "def process_vid_data():\n",
    "    vid_data = pl.read_csv(path/'vid_info/vid_info_table.csv')\n",
    "\n",
    "    (path/'mappings').mkdir(exist_ok=True)\n",
    "    num_columns = [\"item_duration\"]\n",
    "    cat_columns = [col for col in vid_data.columns if col not in num_columns and col != 'vid']\n",
    "    if cat_columns:\n",
    "        vid_data = vid_data.with_columns([\n",
    "            pl.col(col).fill_null(-1.0).cast(pl.Int64)\n",
    "            for col in cat_columns\n",
    "        ])\n",
    "    mapping_dicts = {}\n",
    "    \n",
    "    for col in cat_columns:\n",
    "        # 生成映射字典\n",
    "        unique_values = vid_data[col].unique().to_list()\n",
    "        mapping = {v: i for i, v in enumerate(unique_values)}\n",
    "        mapping_dicts[col] = mapping\n",
    "        \n",
    "        # 应用映射并保存\n",
    "        vid_data = vid_data.with_columns(\n",
    "            pl.col(col).map_elements(lambda x: mapping.get(x, -1), return_dtype=pl.Int64).alias(col)\n",
    "        )\n",
    "        with open(path/f'mappings/{col}_mapping.json', 'w') as f:\n",
    "            json.dump(mapping, f)\n",
    "    norm_params = {}\n",
    "    for col in num_columns:\n",
    "        # 计算均值和标准差\n",
    "        mean_val = vid_data[col].mean()\n",
    "        std_val = vid_data[col].std()\n",
    "        norm_params[col] = {'mean': mean_val, 'std': std_val}\n",
    "        \n",
    "        # 应用标准化\n",
    "        vid_data = vid_data.with_columns(\n",
    "            ((pl.col(col) - mean_val) / std_val).cast(pl.Float32).alias(col+'_n')\n",
    "        )\n",
    "    # 保存正则化参数\n",
    "    param_file = path/'mappings/normalization_params.json'\n",
    "    if param_file.exists():\n",
    "        with open(param_file, 'r') as f:\n",
    "            existing_params = json.load(f)\n",
    "    \n",
    "    # 更新参数（新参数覆盖旧值）\n",
    "    existing_params.update(norm_params)\n",
    "    \n",
    "    # 保存合并后的参数\n",
    "    with open(param_file, 'w') as f:\n",
    "        json.dump(existing_params, f, indent=4)\n",
    "    return vid_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "def create_dataset(start_day=1, end_day=31):\n",
    "    load_data = load_log_data(start_day,end_day)\n",
    "    # 处理用户特征\n",
    "    user_data = process_user_data()\n",
    "    # 处理视频特征\n",
    "    vid_data = process_vid_data()\n",
    "    test_user_data = pl.read_csv(path/\"A榜待预测的did/testA_pred_did.csv\")\n",
    "    test_user_show_data = pl.read_csv(path/\"A榜用户曝光数据/testA_did_show.csv\")\n",
    "\n",
    "    # 构造点击，播放标签 did,vid,click,play\n",
    "    train_data = {}\n",
    "    for day_i in range(start_day,end_day):\n",
    "        view_click_play_data = load_data['view'][day_i].join(load_data['click'][day_i],on=['did','vid'],how='left').join(load_data['play'][day_i],on=['did','vid'],how='left')\n",
    "        train_data_label = view_click_play_data.with_columns(\n",
    "            # pl.when(pl.col('click_time').is_not_null()).then(1).otherwise(0).alias('click'),\n",
    "            pl.col('click_time').is_not_null().alias('click'),\n",
    "            pl.col('play_time').fill_null(pl.lit(0)).alias('play_time')\n",
    "        )\n",
    "        train_data[day_i] = train_data_label.select(['did','vid','click','play_time'])\n",
    "    history = []\n",
    "    for i in range(start_day,end_day):\n",
    "        print(train_data[i].shape)\n",
    "        tmp = train_data[i].join(vid_data, on=\"vid\").join(user_data, on=\"did\")\n",
    "        tmp = tmp.with_columns(\n",
    "            pl.lit(i).alias(\"day\"),\n",
    "            (pl.col(\"play_time\")/pl.col(\"item_duration\")).alias(\"play_ratio\"),\n",
    "            ).join(test_user_data, on=\"did\", how=\"semi\")\n",
    "        history.append(tmp)  \n",
    "\n",
    "    test_data = test_user_show_data.join(vid_data, on=\"vid\").join(user_data, on=\"did\")\n",
    "    test_data = test_data.with_columns(\n",
    "            pl.lit(0).alias(\"click\"),\n",
    "            pl.lit(0).alias(\"play_time\"),\n",
    "            pl.lit(32).alias(\"day\"),\n",
    "            pl.lit(0).alias(\"play_ratio\"),\n",
    "            )\n",
    "    train_data = pl.concat(history)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1492712, 4)\n",
      "(1346235, 4)\n",
      "(1089715, 4)\n",
      "(1363623, 4)\n",
      "(1212882, 4)\n",
      "(1041831, 4)\n",
      "(1260247, 4)\n",
      "(1167650, 4)\n",
      "(889703, 4)\n",
      "(727634, 4)\n",
      "(981486, 4)\n",
      "(1416403, 4)\n",
      "(874771, 4)\n",
      "(956684, 4)\n",
      "(1025837, 4)\n",
      "(764269, 4)\n",
      "(767565, 4)\n",
      "(812498, 4)\n",
      "(760550, 4)\n",
      "(1345987, 4)\n",
      "(1718881, 4)\n",
      "(1232513, 4)\n",
      "(934105, 4)\n",
      "(1006762, 4)\n",
      "(1099988, 4)\n",
      "(833238, 4)\n",
      "(1029554, 4)\n",
      "(1147831, 4)\n",
      "(846025, 4)\n",
      "(757846, 4)\n"
     ]
    }
   ],
   "source": [
    "df,test = create_dataset(1,31)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.filter((pl.col('day')<=29) & (pl.col('day')>=1)).to_pandas()\n",
    "df_valid = df.filter(pl.col('day')==30).to_pandas()\n",
    "# df_test = test.to_pandas()\n",
    "\n",
    "df_train.to_parquet('ctr_df_train_add.parquet',index=False)\n",
    "df_valid.to_parquet('ctr_df_valid_add.parquet',index=False)\n",
    "# df_test.to_parquet('ctr_test.parquet',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3991734 entries, 0 to 3991733\n",
      "Columns: 104 entries, did to play_ratio\n",
      "dtypes: bool(1), float32(38), float64(1), int32(1), int64(62), object(1)\n",
      "memory usage: 2.5+ GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
